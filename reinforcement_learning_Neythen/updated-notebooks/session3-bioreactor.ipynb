{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q network for bioreactor optimisation\n",
    "* Author(s) for paper and code: Neythen J. Treloar\n",
    "* Author(s) for educational material: Miguel Xochicale\n",
    "\n",
    "\n",
    "## Questions\n",
    "* How can an agent learn Chemostat environment that can handle an arbitrary number of bacterial strains?\n",
    "\n",
    "## Objectives\n",
    "* Learn how to use Deep Q network for Chemostat environments\n",
    "\n",
    "## Prerequisites\n",
    "Session 1: Reinforement learning with tabular value functions\n",
    "Session 2: Deep reinforcement learning\n",
    "\n",
    "## 1. Introduciton \n",
    "In this notebook, we demonstrate the key parts of a DQN agent and then apply that to the maximisation of the product output of a microbial co-culture growing in a bioreactor. For full details of the concepts behind this demo, please see [deep reinforcement learning for the control of microbial co-cultures in bioreactors (Treloar et al, 2020)](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007783).\n",
    "\n",
    "Notes. For `DQN_agent()` The configuration variables are similar to those from the session two notebook, with one exception - we introduce `TAU` to enable us to perform soft updates on the parameters of the $ Q_{target} $ network, so that they shift towards the $Q$ network parameters incrementally rather than duplicate them at a single time step. We're also changing the effect of the `UPDATE_EVERY` variable - this now becomes the frequency with which we perform both the gradient descent step on the $Q$ network parameters and the soft update of the $Q_{target}$ parameters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install dependencies\n",
    "import nvidia_smi\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import seaborn.objects as so\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_host_resources():\n",
    "    \"\"\"\n",
    "    Checking RAM and GPU resources\n",
    "    \"\"\"\n",
    "    ## Setting and checking device\n",
    "    ## Checking resources (GPU and Memory)\n",
    "    ####################################\n",
    "    from psutil import virtual_memory\n",
    "    ram_gb = virtual_memory().total / 1e9\n",
    "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "    \n",
    "    # CODESPACES: Your runtime has 8.3 gigabytes of available RAM\n",
    "    # MX_HOST_MACHINE: Your runtime has 33.3 gigabytes of available RAM\n",
    "\n",
    "    _GPU = False\n",
    "    _NUMBER_OF_GPU = 0\n",
    "\n",
    "    #def _check_gpu():\n",
    "    #    global _GPU\n",
    "    #    global _NUMBER_OF_GPU\n",
    "    nvidia_smi.nvmlInit()\n",
    "    _NUMBER_OF_GPU = nvidia_smi.nvmlDeviceGetCount()\n",
    "    if _NUMBER_OF_GPU > 0:\n",
    "        _GPU = True\n",
    "\n",
    "    print(f'GPU = {_GPU}')\n",
    "\n",
    "    ####################################\n",
    "    def _bytes_to_megabytes(bytes):\n",
    "        return round((bytes/1024)/1024,2)\n",
    "\n",
    "    for i in range(_NUMBER_OF_GPU):\n",
    "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(i)\n",
    "        info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "        print(f'GPU-{i}: GPU-Memory: \\\n",
    "               {_bytes_to_megabytes(info.used)} used /{_bytes_to_megabytes(info.total)} total [MB]')\n",
    "        #print(f'GPU-{i}: GPU-Memory: {info.used}/{info.total} MB')\n",
    "\n",
    "    #CODESPACES /bin/bash: line 1: nvidia-smi: command not found\n",
    "      #OSError: libnvidia-ml.so.1: cannot open shared object file: No such file or directory\n",
    "    #MX_HOST_MACHINE: NVIDIA RTX A200 8192MiB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 33.3 gigabytes of available RAM\n",
      "\n",
      "GPU = True\n",
      "GPU-0: GPU-Memory:                221.0 used /8192.0 total [MB]\n"
     ]
    }
   ],
   "source": [
    "check_host_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 17 15:40:26 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A200...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   47C    P8     5W /  35W |      5MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2280      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "\n",
    "#CODESPACES /bin/bash: line 1: nvidia-smi: command not found\n",
    "#MX_HOST_MACHINE: NVIDIA RTX A200 8192MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Setting and checking device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Represent the agent's policy model\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, layer1_size=64, layer2_size=64):\n",
    "        \"\"\"Build a network that can take a description of an environment's state and \n",
    "        output the value of available actions.\n",
    "        \n",
    "        Params\n",
    "        =======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            layer1_size (int): Number of nodes in first hidden layer\n",
    "            layer2_size (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__() ## calls __init__ method of nn.Module class\n",
    "        self.layer1 = nn.Linear(state_size, layer1_size)\n",
    "        self.layer2 = nn.Linear(layer1_size, layer2_size)\n",
    "        self.layer3 = nn.Linear(layer2_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Map state -> action values.\"\"\"\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\",\n",
    "                                                                \"action\",\n",
    "                                                                \"reward\",\n",
    "                                                                \"next_state\",\n",
    "                                                                \"done\"])\n",
    "\n",
    "    def add(self,state, action, reward, next_state,done):\n",
    "        \"\"\"Add a new experience to memory\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory\"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_agent():\n",
    "    \"\"\"Agent that interacts with and learns from an environment using artificial neural networks \n",
    "    to approximate its state-action value function\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 env, \n",
    "                 state_size, \n",
    "                 action_size,\n",
    "                 BUFFER_SIZE = int(1e5),\n",
    "                 BATCH_SIZE = 64,\n",
    "                 GAMMA = 0.99,\n",
    "                 TAU = 1e-3,\n",
    "                 LR = 5e-4,\n",
    "                 UPDATE_EVERY = 4):\n",
    "        \"\"\"Initialize an Agent object\n",
    "\n",
    "        Params\n",
    "        =======\n",
    "            env: an environment object\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "            BATCH_SIZE = 64         # minibatch size\n",
    "            GAMMA = 0.99            # discount factor\n",
    "            TAU = 1e-3              # for soft update of target parameters\n",
    "            LR = 5e-4               # learning rate\n",
    "            UPDATE_EVERY = 4        # how often to update the network\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.BUFFER_SIZE = BUFFER_SIZE\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.GAMMA = GAMMA\n",
    "        self.TAU = TAU\n",
    "        self.LR = LR\n",
    "        self.UPDATE_EVERY = UPDATE_EVERY\n",
    "        \n",
    "        # Function approximation networks:\n",
    "        self.q_network = QNetwork(state_size, action_size).to(device)\n",
    "        self.q_network_target = QNetwork(state_size, action_size).to(device)\n",
    "\n",
    "        # Optimise the parameters in the Q network, using the learning rate defined above\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, self.BUFFER_SIZE, self.BATCH_SIZE)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def get_explore_rate(self, episode, decay):\n",
    "        \"\"\"Calculates the logarithmically decreasing explore rate\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            episode (int): the current episode\n",
    "            decay (float): controls the rate of decay of the explore rate\n",
    "        \n",
    "        Returns\n",
    "        =======\n",
    "            explore_rate (float): the epsilon in the agent's epsilon-greedy policy\n",
    "        \"\"\"\n",
    "\n",
    "        # Input validation\n",
    "        if not 0 < decay:\n",
    "            raise ValueError(\"decay needs to be above 0\")\n",
    "        \n",
    "        # Ensure rate returned is between 0 and 1:\n",
    "        min_explore_rate = 0\n",
    "        max_explore_rate = 1\n",
    "        explore_rate = 1.0 - math.log10(episode / decay)\n",
    "        return max(min_explore_rate, min(max_explore_rate, explore_rate))\n",
    "    \n",
    "    def policy(self, state, epsilon=0):\n",
    "        \"\"\"Returns action for given state as per current policy\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            epsilon (float): for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.q_network.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.q_network(state)\n",
    "        self.q_network.train()\n",
    "\n",
    "        # Epsilon-greedy action selction\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def update_target(self, model, target_model):\n",
    "        \"\"\"Update target model parameters\n",
    "\n",
    "        Params\n",
    "        =======\n",
    "            local model (PyTorch model): weights will be copied from\n",
    "            target model (PyTorch model): weights will be copied to\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), model.parameters()):\n",
    "            target_param.data.copy_(self.TAU*local_param.data + (1-self.TAU)*target_param.data) \n",
    "\n",
    "    def update_Q(self, experiences):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        \n",
    "        Params\n",
    "        =======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # We use mean squared error as the loss function\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        # The local model is the one we need to train so we put it in training mode\n",
    "        self.q_network.train()\n",
    "        # Conversely, we want the target model to be in evaluation mode so that when \n",
    "        # we do a forward pass it does not calculate the gradients\n",
    "        self.q_network_target.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            future_pred = self.q_network_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        # .detach() ->  Returns a new Tensor, detached from the current graph.\n",
    "        targets = rewards + (self.GAMMA * future_pred * (1 - dones))\n",
    "\n",
    "        # Shape of output from the model (batch_size, action_size) \n",
    "        predicted_targets = self.q_network(states).gather(1, actions)\n",
    "\n",
    "        loss = criterion(predicted_targets, targets).to(device)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self, n_episodes=200, max_t=1000, decay=None, verbose=True):\n",
    "        \"\"\"Deep Q-Learning\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            n_episodes (int): maximum number of training epsiodes\n",
    "            max_t (int): maximum number of timesteps per episode\n",
    "            decay (float): controls the rate of decay of the explore rate\n",
    "            verbose (bool): whether to print updates on the training process\n",
    "        \n",
    "        Returns\n",
    "        =======\n",
    "            returns (list[float]): episode returns for analysis of training performance\n",
    "        \"\"\"\n",
    "        returns = [] # list containing total reward from each episode\n",
    "\n",
    "        # Reasonable default value for explore_rate decay:\n",
    "        if not decay:\n",
    "            decay = n_episodes / 11\n",
    "\n",
    "        for episode in range(1, n_episodes+1):\n",
    "            explore_rate = self.get_explore_rate(episode, decay)\n",
    "            state, prob = self.env.reset()\n",
    "            episode_return = 0\n",
    "\n",
    "            for t in range(max_t): \n",
    "                action = self.policy(state, explore_rate)\n",
    "                next_state, reward, done, info, prob = self.env.step(action)\n",
    "\n",
    "                self.memory.add(state, action, reward, next_state, done)\n",
    "                # If enough samples are available in memory, get random subset and learn:\n",
    "                if len(self.memory) > self.BATCH_SIZE and t % self.UPDATE_EVERY == 0:\n",
    "                    experience = self.memory.sample()\n",
    "                    self.update_Q(experience)\n",
    "                    self.update_target(self.q_network, self.q_network_target)\n",
    "                state = next_state\n",
    "                episode_return += reward\n",
    "                if done:\n",
    "                    break\n",
    "        \n",
    "            returns.append(episode_return)\n",
    "            # If verbose mode is switched on, log returns every 10 episodes:\n",
    "            if verbose and episode % 10 == 0:\n",
    "                print(f'Episode {episode}\\tExplore rate {explore_rate:.2f}\\tReturn {episode_return:.2f}')\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioreactorEnv():\n",
    "    '''\n",
    "    Chemostat environment that can handle an arbitrary number of bacterial strains where all are being controlled\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 xdot, \n",
    "                 reward_func, \n",
    "                 sampling_time, \n",
    "                 num_controlled_species, \n",
    "                 initial_x, \n",
    "                 max_t, \n",
    "                 n_states = 10, \n",
    "                 n_actions = 2, \n",
    "                 continuous_s = False):\n",
    "        '''\n",
    "        Parameters:\n",
    "            xdot: array of the derivatives for all state variables\n",
    "            reward_func: function to calculate reward: reward = reward_func(state, action, next_state)\n",
    "            sampling_time: time between sampl-and-hold intervals\n",
    "            num_controlled_species: 2\n",
    "            initial_x: the initial state (e.g., shape array (8,))\n",
    "            max_t: maximum number of timesteps per episode\n",
    "            n_states = 10\n",
    "            n_actions = 2\n",
    "            continuous_s=\n",
    "\t\t\tTrue:  get_state self.xs[-1][0:self.num_controlled_species]/100000\n",
    "\t\t\tFalse: get_state = self.pop_to_state(self.xs[-1][0:self.num_controlled_species])\n",
    "        Returns:\n",
    "            env returns populations/scaling to agent\n",
    "        References:\n",
    "            https://github.com/ucl-cssb/ROCC/blob/master/ROCC/chemostat_env/chemostat_envs.py\n",
    "        '''\n",
    "        one_min = 0.016666666667 #(1/60)\n",
    "        self.scaling = 1 #population scaling to prevent neural network instability in agent, aim to have pops between 0 and 1. \n",
    "        self.xdot = xdot\n",
    "        self.xs = [] # append odeint solutions of xdot\n",
    "        self.us = [] # append actions\n",
    "        self.sampling_time = sampling_time*one_min\n",
    "        self.reward_func = reward_func\n",
    "\n",
    "        self.u_bounds = [0,0.1]\n",
    "        self.N_bounds = [0, 50000]\n",
    "\n",
    "        self.u_disc = n_actions\n",
    "        self.N_disc = n_states\n",
    "        self.num_controlled_species = num_controlled_species\n",
    "        self.initial_x = initial_x\n",
    "        self.max_t = max_t\n",
    "        self.continuous_s = continuous_s\n",
    "    \n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Performs one sampling and hold interval using the action provided by a reinforcment learning agent\n",
    "\n",
    "        Parameters:\n",
    "            action: action chosen by agent\n",
    "        Returns:\n",
    "            state: scaled state to be observed by agent\n",
    "            reward: reward obtained buring this sample-and-hold interval\n",
    "            done: boolean value indicating whether the environment has reached a terminal state\n",
    "        '''\n",
    "        \n",
    "        u = self.action_to_u(action)\n",
    "        \n",
    "        #add noise\n",
    "        #Cin = np.random.normal(Cin, 0.1*Cin) #10% pump noise\n",
    "\n",
    "        self.us.append(u)\n",
    "\n",
    "        ts = [0, self.sampling_time]\n",
    "        sol = odeint(self.xdot, self.xs[-1], ts, args=(u,))[1:]\n",
    "        self.xs.append(sol[-1,:])\n",
    "        self.state = self.get_state() #scaled bacterial populations\n",
    "        reward, done = self.reward_func(self.xs[-1]) #reward func with last appended sol from 0 to max_t\n",
    "        \n",
    "        if len(self.xs) == self.max_t:\n",
    "            done = True\n",
    "\n",
    "        return self.state, reward, done, None, 1\n",
    "\n",
    "    def action_to_u(self,action):\n",
    "        '''\n",
    "        Takes a discrete action index and returns the corresponding continuous state vector\n",
    "\n",
    "        Paremeters:\n",
    "            action: the descrete action\n",
    "            num_species: the number of bacterial populations\n",
    "            num_Cin_states: the number of action states the agent can choose from for each species\n",
    "            Cin_bounds: list of the upper and lower bounds of the Cin states that can be chosen\n",
    "        Returns:\n",
    "            state: the continuous Cin concentrations correspoding to the chosen action\n",
    "        '''\n",
    "\n",
    "        # calculate which bucket each eaction belongs in\n",
    "        buckets = np.unravel_index(action, [self.u_disc] * self.num_controlled_species)\n",
    "\n",
    "        # convert each bucket to a continuous state variable\n",
    "        u = []\n",
    "        for r in buckets:\n",
    "            u.append(self.u_bounds[0] + r*(self.u_bounds[1]-self.u_bounds[0])/(self.u_disc-1))\n",
    "\n",
    "        u = np.array(u).reshape(self.num_controlled_species,)\n",
    "        return np.clip(u, self.u_bounds[0], self.u_bounds[1])\n",
    "\n",
    "\n",
    "    def get_state(self):\n",
    "        '''\n",
    "        Gets the state (scaled bacterial populations) to be observed by the agent\n",
    "\n",
    "        Returns:\n",
    "            scaled bacterial populations (1/100000). E.g.,:`[0.26154319 0.2205354 ]`\n",
    "        '''\n",
    "        if self.continuous_s:\n",
    "            return self.xs[-1][0:self.num_controlled_species]/100000\n",
    "        else:\n",
    "            return self.pop_to_state(self.xs[-1][0:self.num_controlled_species])\n",
    "\n",
    "    \n",
    "    def pop_to_state(self, N):\n",
    "        '''\n",
    "        discritises the population of bacteria to a state suitable for the agent\n",
    "\n",
    "        :param N: population\n",
    "        :return: discitised population\n",
    "        '''\n",
    "        step = (self.N_bounds[1] - self.N_bounds[0])/self.N_disc\n",
    "        N = np.clip(N, self.N_bounds[0], self.N_bounds[1]-1)\n",
    "        return np.ravel_multi_index((N//step).astype(np.int32), [self.N_disc]*self.num_controlled_species)\n",
    "\n",
    "    def reset(self, initial_x = None):\n",
    "        '''\n",
    "        Resets env to inital state:\n",
    "\n",
    "        Parameters:\n",
    "            initial_S (optional) the initial state to be reset to if different to the default\n",
    "        Returns:\n",
    "            The state to be observed by the agent\n",
    "        '''\n",
    "        \n",
    "        if initial_x is None:\n",
    "            initial_x = self.initial_x\n",
    "\n",
    "        self.xs = [initial_x]\n",
    "        self.us = []\n",
    "        return (self.get_state(),1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Bioreactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_257224/1395901787.py:71: ODEintWarning: Excess work done on this call (perhaps wrong Dfun type). Run with full_output = 1 to get quantitative information.\n",
      "  sol = odeint(self.xdot, self.xs[-1], ts, args=(u,))[1:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tExplore rate 0.26\tReturn 39.10\n",
      "Episode 20\tExplore rate 0.00\tReturn 26.47\n",
      "---------------------------------\n",
      "Execution time (minutes): 0.41586956977844236\n"
     ]
    }
   ],
   "source": [
    "current_time = time.time()\n",
    "\n",
    "\n",
    "def monod(C, C0, umax, Km, Km0):\n",
    "    '''\n",
    "    Calculates the growth rate based on the monod equation\n",
    "\n",
    "    Parameters:\n",
    "        C: the concetrations of the auxotrophic nutrients for each bacterial\n",
    "            population\n",
    "        C0: concentration of the common carbon source\n",
    "        Rmax: array of the maximum growth rates for each bacteria\n",
    "        Km: array of the saturation constants for each auxotrophic nutrient\n",
    "        Km0: array of the saturation constant for the common carbon source for\n",
    "            each bacterial species\n",
    "    '''\n",
    "\n",
    "    # convert to numpy\n",
    "\n",
    "    growth_rate = ((umax * C) / (Km + C)) * (C0 / (Km0 + C0))\n",
    "\n",
    "    return growth_rate\n",
    "\n",
    "def xdot_product(x, t, u):\n",
    "    '''\n",
    "    Calculates and returns derivatives for the numerical solver odeint\n",
    "\n",
    "    Parameters:\n",
    "        x: current state (e.g., xdot.shape = (8,))\n",
    "        t: current time\n",
    "        u: array of the concentrations of the auxotrophic nutrients and the common carbon source\n",
    "        #num_species: the number of bacterial populations\n",
    "    Returns:\n",
    "        xdot: array of the derivatives for all state variables\n",
    "    References:\n",
    "        https://github.com/ucl-cssb/ROCC/blob/master/ROCC/chemostat_env/chemostat_envs.py        \n",
    "    '''\n",
    "    q = 0.5 #(0-Umax)\n",
    "    \n",
    "    y, y0, umax, Km, Km0 = [np.array(x) for x in [\n",
    "                            [480000., 480000.], # y (10**12)\n",
    "                            [520000., 520000.], # y0 (10**12)\n",
    "                            [1., 1.1], # Umax (0.4 - 3)\n",
    "                            [0.00048776, 0.000000102115],   # Km (2)\n",
    "                            [0.00006845928, 0.00006845928]]  # Km0 (2)\n",
    "                           ]\n",
    "    \n",
    "    # extract variables\n",
    "    N = x[:2] #np.array(S[:self.num_species])\n",
    "    C = x[2:4] #np.array(S[self.num_species:self.num_species+self.num_controlled_species])\n",
    "    C0 = x[4] # np.array(S[-1])\n",
    "    A = x[5]\n",
    "    B = x[6]\n",
    "    P = x[7]\n",
    "\n",
    "    R = monod(C, C0, umax, Km, Km0)\n",
    "\n",
    "    # calculate derivatives\n",
    "    dN = N * (R - q)  # q term takes account of the dilution\n",
    "    dC = q * (u - C) - (1 / y) * R * N # sometimes dC.shape is (2,2)\n",
    "    dC0 = q*(0.1 - C0) - sum(1/y0[i]*R[i]*N[i] for i in range(2)) # Eq1. concentration of the shared carbon source\n",
    "\n",
    "    dA = N[0] - 2 * A ** 2 * B - q * A\n",
    "    dB = N[1] - A ** 2 * B - q * B\n",
    "    dP = A ** 2 * B - q * P\n",
    "\n",
    "    # consstruct derivative vector for odeint\n",
    "    xdot = np.append(dN, dC)\n",
    "    xdot = np.append(xdot, dC0)\n",
    "    xdot = np.append(xdot, dA)\n",
    "    xdot = np.append(xdot, dB)\n",
    "    xdot = np.append(xdot, dP)\n",
    "    \n",
    "    return xdot\n",
    "\n",
    "\n",
    "\n",
    "def reward_function(x):\n",
    "    \"\"\"\n",
    "    caluclates the reward based on the rate of product output\n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    P = x[-1]\n",
    "\n",
    "    if x[0] < 1000 or x[1] < 1000:\n",
    "        reward = -1\n",
    "        done = True\n",
    "    else:\n",
    "        reward = P/100000\n",
    "        done = False\n",
    "\n",
    "    return reward, done\n",
    "\n",
    "\n",
    "\n",
    "num_controlled_species = 2\n",
    "sampling_time = 10  # minutes\n",
    "t_steps = int((24 * 60) / sampling_time)  # set this to 24 hours\n",
    "initial_x = np.array([20000, 30000, 0., 0., 1., 0., 0., 0.]) # the initial state\n",
    "\n",
    "n_states_env = 2\n",
    "n_actions_env = 4\n",
    "\n",
    "## Setting uo chemostat environment\n",
    "env = BioreactorEnv(xdot_product, \n",
    "                    reward_function, \n",
    "                    sampling_time,\n",
    "                    num_controlled_species, \n",
    "                    initial_x, \n",
    "                    t_steps, \n",
    "                    #n_states_env, #default: n_states = 10, \n",
    "                    #n_actions_env, #default: n_actions = 2, \n",
    "                    continuous_s = True)  \n",
    "\n",
    "## Setting up DQN_agent\n",
    "n_states = 2\n",
    "n_actions = 4\n",
    "agent = DQN_agent(env, n_states, n_actions)\n",
    "# n_episodes = 1000 #Original\n",
    "n_episodes = 20\n",
    "\n",
    "## Train DQN_agent\n",
    "returns = agent.train(n_episodes)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'---------------------------------')\n",
    "print(f'Execution time (minutes): {(end_time - current_time)/60}')\n",
    "#logs\n",
    "##Execution time (mins): 0.4359638055165609 with n_episodes 20 in MX_HOST_MACHINE: 33.3G RAM, NVIDIA RTX A200 8192MiB\n",
    "##Execution time (mins): 0.3237577478090922 with n_episodes 20 in CODESPACES  2-core • 8GB RAM • 32GB HD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_rates = [\n",
    "    agent.get_explore_rate(episode, 1.5) for episode in range(1, n_episodes+1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting and Saving plots\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.plot(returns, label='Return')\n",
    "explore_rates = [agent.get_explore_rate(episode, n_episodes / 11) for episode in range(1, n_episodes+1)]\n",
    "ax1.set_ylabel('Return')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(explore_rates, color='black', label='Explore rate')\n",
    "ax2.set_ylabel('Explore Rate')\n",
    "ax2.set_xlabel('Episode')\n",
    "plt.tight_layout()\n",
    "ax1.legend(loc=(0.21, 0.67))\n",
    "ax2.legend(loc=(0.6, 0.22))\n",
    "plt.savefig('fig-return_explore_rate.png')\n",
    "plt.close()\n",
    "#plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Final population curve')\n",
    "plt.plot(np.arange(len(env.xs)) *sampling_time, [x[0] for x in env.xs], label = '$N_1$')\n",
    "plt.plot(np.arange(len(env.xs)) *sampling_time, [x[1] for x in env.xs], label = '$N_2$')\n",
    "plt.legend()\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('Population cells/L')\n",
    "plt.savefig('fig-population_cells.png')\n",
    "plt.close()\n",
    "#plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(2,1)\n",
    "plt.title('Actions')\n",
    "axs[0].step(np.arange(len(env.us)) * sampling_time, [x[0] for x in env.us], label = '$u_1$')\n",
    "axs[1].step(np.arange(len(env.us)) * sampling_time, [x[1] for x in env.us], label = '$u_2$', color = 'orange')\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('$C_{in}$')\n",
    "plt.savefig('fig-actions.png')\n",
    "plt.close()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
