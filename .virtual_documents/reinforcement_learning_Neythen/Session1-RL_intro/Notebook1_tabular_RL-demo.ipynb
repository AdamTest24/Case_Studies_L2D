





import os
from pathlib import Path
import sys

lib_dir = Path(os.getcwd()).parent / 'lib'
sys.path.append(str(lib_dir))

from agents_nt import MC_agent, TD_agent
from utils_nt import plot_value, plot_returns, plot_explore
import numpy as np
import matplotlib.pyplot as plt

import gymnasium as gym
from IPython import display as ipythondisplay

import time    





env = gym.make('CliffWalking-v0')





n_states = 48
grid_shape = (4, 12) #Â (Rows, columns)

n_actions = 4
p_map = {0:'U', 1:'R', 2:'D', 3:'L'} # Up, right, down, left








class TD_agent():
    def __init__(self, n_states, n_actions, gamma=0.95, alpha=0.01):
        self.gamma = gamma
        self.alpha = alpha
        self.n_states = n_states
        self.n_actions = n_actions
        self.Q_func = np.zeros((n_states, n_actions))





def train_agent(agent, env,  n_episodes = 10001, explore_rate = 0.05, monte_carlo = False, display=False, simple_text = False):

    returns = []
    
    for episode in range(n_episodes): # for each episode

        e_return = 0 # sum the reward we get this episode
        e_transitions = [] # memory of all transitions seen in this episode
        done = False # has the episode finished?
        
        state, prob = env.reset() # reset env to initial state

        action = agent.policy(state, explore_rate) # initialise the agent

        if display: # display the current agent-evironment state
            #env.render()
            plt.imshow(env.render())
            if simple_text:
              time.sleep(0.1)
            ipythondisplay.clear_output(wait=True)
            ipythondisplay.display(plt.gcf())
        
        
        while not done: # run the episode until a terminal state reached
 
            next_state, reward, done, info, prob = env.step(action) # take an action and get the resulting state from the env
          
            next_action = agent.policy(next_state, explore_rate) # get the next action to apply from agent's policy
            transition = (state, action, reward, next_state, next_action, done) # create the SARSA transition
            e_transitions.append(transition) # add to the memory

            if not monte_carlo: # update the Q function of temporal difference agents
                agent.update_Q(transition)

            state = next_state
            action = next_action

            e_return += reward
            
            if display: # display the current agent-evironment state
                #print(env.render())
                plt.imshow(env.render())
                time.sleep(0.1)
                ipythondisplay.clear_output(wait=True)
                ipythondisplay.display(plt.gcf())


        returns.append(e_return)
        
        if monte_carlo: # update the q function of a monte carlo agent
            agent.update_Q(e_transitions)

        if episode % 1000 == 0: # print results of current episode
            print('episode:', episode, ', explore_rate:', explore_rate, ', return:', e_return)
    if display:
      ipythondisplay.clear_output(wait=True) 
    return returns
        





def epsilon_greedy_policy(self, state, explore_rate):
    '''
    chooses an action based on the agents value function and the current explore rate
    :param state: the current state given by the environment
    :param explore_rate: the chance of taking a random action
    :return: the action to be taken
    '''


    if np.random.random() < explore_rate:
        action = np.random.choice(range(self.n_actions))
    else:
        action = np.argmax(self.Q_func[state])

    return action

TD_agent.policy = epsilon_greedy_policy





def SARSA_update(self,transition):
    '''
    updates the agents value function based on the experience in transition
    :param transition:
    :return:
    '''

    state, action, reward, next_state, next_action, done = transition

    self.Q_func[state, action] += self.alpha * (reward + self.gamma * self.Q_func[next_state,next_action] * (1 - done) - self.Q_func[state, action])

TD_agent.update_Q = SARSA_update





SARSA_agent = TD_agent(n_states,n_actions)
print('TRAINING')
returns = train_agent(SARSA_agent, env, explore_rate = 0.05) # train the agent







# run a test episode
print()
print('TEST EPISODE')
r = train_agent(SARSA_agent, env, n_episodes=1, explore_rate=0, display=True, simple_text = True) # run a test episode to get the learned behaviour with no exploring






# plot value function, policy and return 
plot_value(SARSA_agent.Q_func, grid_shape, p_map) # plot the value function and policy 
plot_returns(returns)








def Q_update(self,transition):
    '''
    updates the agents value function based on the experience in transition
    :param transition:
    :return:
    '''

    state, action, reward, next_state, next_action, done = transition

    self.Q_func[state, action] += self.alpha * (reward + self.gamma * np.max(self.Q_func[next_state]) * (1 - done) - self.Q_func[state, action])

TD_agent.update_Q = Q_update




Q_agent = TD_agent(n_states, n_actions)
print('TRAINING')
returns = train_agent(Q_agent, env, explore_rate = 0.05) # train the agent


# run a test episode
print()
print('TEST EPISODE')
r = train_agent(Q_agent, env, n_episodes=1, explore_rate=0, display=False, simple_text = True) # run a test episode to get the learned behaviour with no exploring



# plot value function, policy and return 
plot_value(Q_agent.Q_func, grid_shape, p_map) # plot the value function and policy 
plot_returns(returns)








# train the agent 
M_agent = MC_agent(n_states, n_actions)
returns = train_agent(M_agent, env,n_episodes=10001,  monte_carlo=True, explore_rate=0.3)



# run test episode
r = train_agent(M_agent, env, n_episodes=1, explore_rate=0, display=False, monte_carlo=True, simple_text = True)



#plot value function, policy and return
plot_value(M_agent.Q_func, grid_shape, p_map)
plot_returns(returns)



